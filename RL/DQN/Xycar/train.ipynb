{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4e61d2c1fad64cac47e95bb3c0ea4a47f70b16ea8e6e402829d27873672fc9e6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dqn import *\n",
    "from simulator.simulator import Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_frame = 1\n",
    "stack_frame = 5\n",
    "\n",
    "epsilon_init = 0.3\n",
    "epsilon_decay = 0.0001\n",
    "\n",
    "learning_rate = 0.0001\n",
    "discount_factor = 0.99\n",
    "batch_size = 512\n",
    "\n",
    "max_step = 5000\n",
    "max_episode = 10000\n",
    "\n",
    "save_episode = 10\n",
    "update_target_episode = 20\n",
    "\n",
    "comment = \"break_-1_reverse_-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(8, stack_frame, 9)\n",
    "target_model = NN(8, stack_frame, 9)\n",
    "agent = DQNAgent(model, target_model, learning_rate, epsilon_init, skip_frame, stack_frame)\n",
    "\n",
    "# agent.model_load(1750, eval=False)\n",
    "# agent.epsilon = 0.278\n",
    "\n",
    "env = Simulator()\n",
    "writer = SummaryWriter(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "REVERSE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "REVERSE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "REVERSE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "REVERSE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "REVERSE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "REVERSE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "REVERSE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "REVERSE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "REVERSE\n",
      "REVERSE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "REVERSE\n",
      "REVERSE\n",
      "DRIVE\n",
      "DRIVE\n",
      "REVERSE\n",
      "REVERSE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n",
      "DRIVE\n"
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "\n",
    "try:\n",
    "    while episode < max_episode:\n",
    "        step = 0\n",
    "\n",
    "        losses = []\n",
    "        max_qs = []\n",
    "        rewards = []\n",
    "        \n",
    "        obs, _ = env.reset()\n",
    "        agent.reset(obs)\n",
    "\n",
    "        state = agent.skip_stack_frame(obs)\n",
    "\n",
    "        while not env.is_done or step < max_step:\n",
    "            env.render()\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # 조향각 조정\n",
    "            if action % 3 == 0:\n",
    "                steering_deg = -env.max_steering_deg\n",
    "            elif action % 3 == 1:\n",
    "                steering_deg = 0\n",
    "            elif action % 3 == 2:\n",
    "                steering_deg = env.max_steering_deg\n",
    "            \n",
    "            # 기어 조정\n",
    "            if action // 3 == 0:\n",
    "                gear = env.DRIVE\n",
    "                reward = 0\n",
    "                print(\"DRIVE\")\n",
    "            elif action // 3 == 1:\n",
    "                gear = env.BREAK\n",
    "                reward = -1\n",
    "            elif action // 3 == 2:\n",
    "                gear = env.REVERSE\n",
    "                print(\"REVERSE\")\n",
    "                reward = -2\n",
    "\n",
    "            next_obs, done = env.step(gear, steering_deg)\n",
    "            next_state = agent.skip_stack_frame(next_obs)\n",
    "            done = 1 if done else 0\n",
    "\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "\n",
    "            if len(agent.experience_memory) >= 1024:\n",
    "                for _ in range(10):\n",
    "                    loss, max_q = agent.train_model(discount_factor, batch_size)\n",
    "                    losses.append(loss)\n",
    "                    max_qs.append(max_q)\n",
    "\n",
    "            if agent.epsilon > agent.epsilon_min:\n",
    "                agent.epsilon -= epsilon_decay\n",
    "\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "            step += 1\n",
    "\n",
    "        print(\"episode: {:d} | loss: {:.4f} | reward: {} | epsilon: {} | memory size: {:d}\".format(\n",
    "            episode, np.mean(losses), np.sum(rewards), agent.epsilon, len(agent.experience_memory)))\n",
    "\n",
    "        writer.add_scalar(\"DQN/loss\", np.mean(losses), episode)\n",
    "        writer.add_scalar(\"DQN/max_q\", np.mean(max_qs), episode)\n",
    "        writer.add_scalar(\"DQN/reward\", np.sum(rewards), episode)\n",
    "        writer.add_scalar(\"DQN/epsilon\", agent.epsilon, episode)\n",
    "        writer.flush()\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        # target 네트워크 업데이트\n",
    "        if episode % update_target_episode == 0:\n",
    "            agent.update_target()\n",
    "        \n",
    "        # 모델 저장\n",
    "        if episode % save_episode == 0:\n",
    "            agent.model_save(episode, comment)\n",
    "\n",
    "finally:\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}